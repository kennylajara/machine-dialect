"""Performance benchmarking for MIR compilation pipeline.

This module provides tools to measure and track compilation performance,
including time, memory usage, and bytecode metrics.
"""

from __future__ import annotations

import gc
import time
import tracemalloc
from collections.abc import Callable
from dataclasses import dataclass

from machine_dialect.ast import Expression, Program
from machine_dialect.mir.hir_to_mir import lower_to_mir
from machine_dialect.mir.mir_module import MIRModule
from machine_dialect.mir.mir_to_bytecode import generate_bytecode


@dataclass
class BenchmarkResult:
    """Results from a benchmark run.

    Attributes:
        name: Benchmark name.
        compilation_time: Total compilation time in seconds.
        ast_to_mir_time: Time for AST to MIR lowering.
        mir_to_bytecode_time: Time for MIR to bytecode generation.
        memory_peak: Peak memory usage in bytes.
        memory_allocated: Total memory allocated.
        bytecode_size: Size of generated bytecode.
        instruction_count: Number of bytecode instructions.
        constant_count: Number of constants in pool.
        mir_instruction_count: Number of MIR instructions.
    """

    name: str
    compilation_time: float
    ast_to_mir_time: float
    mir_to_bytecode_time: float
    memory_peak: int
    memory_allocated: int
    bytecode_size: int
    instruction_count: int
    constant_count: int
    mir_instruction_count: int

    def __str__(self) -> str:
        """Return formatted benchmark results."""
        return f"""
Benchmark: {self.name}
=====================================
Compilation Time:     {self.compilation_time:.4f}s
  - AST to MIR:      {self.ast_to_mir_time:.4f}s ({self.ast_to_mir_time/self.compilation_time*100:.1f}%)
  - MIR to Bytecode: {self.mir_to_bytecode_time:.4f}s ({self.mir_to_bytecode_time/self.compilation_time*100:.1f}%)

Memory Usage:
  - Peak:            {self.memory_peak / 1024:.2f} KB
  - Allocated:       {self.memory_allocated / 1024:.2f} KB

Output Metrics:
  - Bytecode Size:   {self.bytecode_size} bytes
  - Instructions:    {self.instruction_count}
  - Constants:       {self.constant_count}
  - MIR Instructions:{self.mir_instruction_count}

Performance Ratios:
  - Bytes/Instruction: {self.bytecode_size / max(1, self.instruction_count):.2f}
  - MIR/Bytecode Ratio: {self.mir_instruction_count / max(1, self.instruction_count):.2f}
"""


class CompilationBenchmark:
    """Benchmark compilation performance."""

    def __init__(self) -> None:
        """Initialize the benchmark."""
        self.results: list[BenchmarkResult] = []

    def benchmark_compilation(self, program: Program, name: str = "unnamed") -> BenchmarkResult:
        """Benchmark the compilation of a program.

        Args:
            program: The AST program to compile.
            name: Name for this benchmark.

        Returns:
            Benchmark results.
        """
        # Force garbage collection before benchmark
        gc.collect()

        # Start memory tracking
        tracemalloc.start()
        memory_before = tracemalloc.get_traced_memory()

        # Measure AST to MIR
        start_total = time.perf_counter()
        start_ast_to_mir = time.perf_counter()

        mir_module = lower_to_mir(program)

        end_ast_to_mir = time.perf_counter()
        ast_to_mir_time = end_ast_to_mir - start_ast_to_mir

        # Count MIR instructions
        mir_instruction_count = self._count_mir_instructions(mir_module)

        # Measure MIR to bytecode
        start_mir_to_bytecode = time.perf_counter()

        bytecode_module = generate_bytecode(mir_module)

        end_mir_to_bytecode = time.perf_counter()
        mir_to_bytecode_time = end_mir_to_bytecode - start_mir_to_bytecode

        end_total = time.perf_counter()
        compilation_time = end_total - start_total

        # Get memory usage
        memory_current, memory_peak = tracemalloc.get_traced_memory()
        tracemalloc.stop()

        # Calculate bytecode metrics
        bytecode_size = len(bytecode_module.main_chunk.bytecode)
        instruction_count = self._count_instructions(list(bytecode_module.main_chunk.bytecode))
        constant_count = bytecode_module.main_chunk.constants.size()

        # Calculate memory allocated
        memory_allocated = memory_current - memory_before[0]

        result = BenchmarkResult(
            name=name,
            compilation_time=compilation_time,
            ast_to_mir_time=ast_to_mir_time,
            mir_to_bytecode_time=mir_to_bytecode_time,
            memory_peak=memory_peak,
            memory_allocated=memory_allocated,
            bytecode_size=bytecode_size,
            instruction_count=instruction_count,
            constant_count=constant_count,
            mir_instruction_count=mir_instruction_count,
        )

        self.results.append(result)
        return result

    def benchmark_function(
        self, func: Callable[[], Program], name: str = "unnamed", iterations: int = 100
    ) -> BenchmarkResult:
        """Benchmark compilation of a program generated by a function.

        Args:
            func: Function that generates an AST program.
            name: Name for this benchmark.
            iterations: Number of iterations to run.

        Returns:
            Average benchmark results.
        """
        # Warm-up run
        program = func()
        _ = generate_bytecode(lower_to_mir(program))

        # Collect results
        times = []
        memory_peaks = []

        for _ in range(iterations):
            gc.collect()
            program = func()

            tracemalloc.start()
            start = time.perf_counter()

            mir_module = lower_to_mir(program)
            bytecode_module = generate_bytecode(mir_module)

            end = time.perf_counter()
            _, peak = tracemalloc.get_traced_memory()
            tracemalloc.stop()

            times.append(end - start)
            memory_peaks.append(peak)

        # Use the last generated modules for metrics
        mir_instruction_count = self._count_mir_instructions(mir_module)
        bytecode_size = len(bytecode_module.main_chunk.bytecode)
        instruction_count = self._count_instructions(list(bytecode_module.main_chunk.bytecode))
        constant_count = bytecode_module.main_chunk.constants.size()

        # Calculate averages
        avg_time = sum(times) / len(times)
        avg_memory = sum(memory_peaks) / len(memory_peaks)

        result = BenchmarkResult(
            name=f"{name} (avg of {iterations})",
            compilation_time=avg_time,
            ast_to_mir_time=avg_time * 0.4,  # Approximate split
            mir_to_bytecode_time=avg_time * 0.6,
            memory_peak=int(avg_memory),
            memory_allocated=int(avg_memory * 0.8),  # Approximate
            bytecode_size=bytecode_size,
            instruction_count=instruction_count,
            constant_count=constant_count,
            mir_instruction_count=mir_instruction_count,
        )

        self.results.append(result)
        return result

    def compare_results(self, result1: BenchmarkResult, result2: BenchmarkResult) -> str:
        """Compare two benchmark results.

        Args:
            result1: First result (baseline).
            result2: Second result (comparison).

        Returns:
            Formatted comparison.
        """
        time_diff = (result2.compilation_time - result1.compilation_time) / result1.compilation_time * 100
        memory_diff = (result2.memory_peak - result1.memory_peak) / result1.memory_peak * 100
        size_diff = (result2.bytecode_size - result1.bytecode_size) / result1.bytecode_size * 100

        faster_slower = "slower" if time_diff > 0 else "faster"
        more_less_memory = "more" if memory_diff > 0 else "less"
        larger_smaller = "larger" if size_diff > 0 else "smaller"

        return f"""
Comparison: {result1.name} vs {result2.name}
=====================================
Time:       {abs(time_diff):.1f}% {faster_slower}
Memory:     {abs(memory_diff):.1f}% {more_less_memory}
Bytecode:   {abs(size_diff):.1f}% {larger_smaller}

Detailed Comparison:
--------------------
                    Baseline    Comparison    Diff
Compilation Time:   {result1.compilation_time:.4f}s    {result2.compilation_time:.4f}s    {time_diff:+.1f}%
Memory Peak:        {result1.memory_peak/1024:.1f}KB    {result2.memory_peak/1024:.1f}KB    {memory_diff:+.1f}%
Bytecode Size:      {result1.bytecode_size}B    {result2.bytecode_size}B    {size_diff:+.1f}%
Instructions:       {result1.instruction_count}    {result2.instruction_count}    \
{(result2.instruction_count - result1.instruction_count):+d}
"""

    def generate_report(self) -> str:
        """Generate a summary report of all benchmarks.

        Returns:
            Formatted report.
        """
        if not self.results:
            return "No benchmark results available."

        report = "Compilation Benchmark Report\n"
        report += "=" * 50 + "\n\n"

        # Summary statistics
        avg_time = sum(r.compilation_time for r in self.results) / len(self.results)
        avg_memory = sum(r.memory_peak for r in self.results) / len(self.results)
        avg_size = sum(r.bytecode_size for r in self.results) / len(self.results)

        report += f"Summary ({len(self.results)} benchmarks):\n"
        report += f"  Average Compilation Time: {avg_time:.4f}s\n"
        report += f"  Average Memory Peak:      {avg_memory/1024:.2f}KB\n"
        report += f"  Average Bytecode Size:    {avg_size:.1f} bytes\n\n"

        # Individual results
        report += "Individual Results:\n"
        report += "-" * 50 + "\n"

        for result in self.results:
            report += f"\n{result.name}:\n"
            report += f"  Time: {result.compilation_time:.4f}s | "
            report += f"Memory: {result.memory_peak/1024:.1f}KB | "
            report += f"Size: {result.bytecode_size}B | "
            report += f"Instructions: {result.instruction_count}\n"

        # Performance analysis
        if len(self.results) > 1:
            report += "\n" + "Performance Analysis:\n"
            report += "-" * 50 + "\n"

            fastest = min(self.results, key=lambda r: r.compilation_time)
            slowest = max(self.results, key=lambda r: r.compilation_time)
            smallest = min(self.results, key=lambda r: r.bytecode_size)
            largest = max(self.results, key=lambda r: r.bytecode_size)

            report += f"  Fastest:  {fastest.name} ({fastest.compilation_time:.4f}s)\n"
            report += f"  Slowest:  {slowest.name} ({slowest.compilation_time:.4f}s)\n"
            report += f"  Smallest: {smallest.name} ({smallest.bytecode_size} bytes)\n"
            report += f"  Largest:  {largest.name} ({largest.bytecode_size} bytes)\n"

        return report

    def _count_mir_instructions(self, module: MIRModule) -> int:
        """Count total MIR instructions in a module.

        Args:
            module: MIR module.

        Returns:
            Total instruction count.
        """
        count = 0
        for func in module.functions.values():
            for block in func.cfg.blocks.values():
                count += len(block.instructions)
        return count

    def _count_instructions(self, bytecode: list[int]) -> int:
        """Count actual instructions in bytecode.

        Args:
            bytecode: Raw bytecode.

        Returns:
            Number of instructions.
        """
        # This is approximate - counts opcodes only
        from machine_dialect.codegen.isa import Opcode

        count = 0
        i = 0
        while i < len(bytecode):
            opcode = bytecode[i]
            count += 1
            i += 1

            # Skip operands based on opcode
            if opcode in [
                Opcode.LOAD_CONST,
                Opcode.LOAD_LOCAL,
                Opcode.STORE_LOCAL,
                Opcode.JUMP,
                Opcode.JUMP_IF_FALSE,
                Opcode.LOAD_GLOBAL,
                Opcode.STORE_GLOBAL,
                Opcode.LOAD_FUNCTION,
            ]:
                i += 2  # 16-bit operand
            elif opcode == Opcode.CALL:
                i += 1  # 8-bit argument count

        return count


def run_standard_benchmarks() -> None:
    """Run a standard set of benchmarks."""
    from machine_dialect.ast import (
        BlockStatement,
        Identifier,
        IfStatement,
        InfixExpression,
        ReturnStatement,
        SetStatement,
        WholeNumberLiteral,
    )
    from machine_dialect.lexer.tokens import Token, TokenType

    def _token(token_type: TokenType, value: str = "") -> Token:
        """Create a token for testing."""
        return Token(token_type, value, 0, 0)

    def _create_infix(left: Expression, op: str, right: Expression) -> InfixExpression:
        """Helper to create InfixExpression properly."""
        token_type = {
            "+": TokenType.OP_PLUS,
            "-": TokenType.OP_MINUS,
            "*": TokenType.OP_STAR,
            "/": TokenType.OP_DIVISION,
            ">": TokenType.OP_GT,
            "<": TokenType.OP_LT,
            ">=": TokenType.OP_GTE,
            "<=": TokenType.OP_LTE,
        }.get(op, TokenType.OP_PLUS)

        token = Token(token_type, op, 0, 0)
        infix = InfixExpression(token, op, left)
        infix.right = right
        return infix

    benchmark = CompilationBenchmark()

    # Simple program
    simple = Program(
        [
            ReturnStatement(
                _token(TokenType.KW_RETURN, "return"),
                WholeNumberLiteral(_token(TokenType.LIT_WHOLE_NUMBER, "42"), 42),
            )
        ]
    )
    benchmark.benchmark_compilation(simple, "Simple Return")

    # Medium complexity
    medium = Program(
        [
            SetStatement(
                _token(TokenType.KW_SET, "set"),
                Identifier(_token(TokenType.MISC_IDENT, "x"), "x"),
                WholeNumberLiteral(_token(TokenType.LIT_WHOLE_NUMBER, "10"), 10),
            ),
            SetStatement(
                _token(TokenType.KW_SET, "set"),
                Identifier(_token(TokenType.MISC_IDENT, "y"), "y"),
                WholeNumberLiteral(_token(TokenType.LIT_WHOLE_NUMBER, "20"), 20),
            ),
            SetStatement(
                _token(TokenType.KW_SET, "set"),
                Identifier(_token(TokenType.MISC_IDENT, "z"), "z"),
                _create_infix(
                    Identifier(_token(TokenType.MISC_IDENT, "x"), "x"),
                    "+",
                    Identifier(_token(TokenType.MISC_IDENT, "y"), "y"),
                ),
            ),
            ReturnStatement(
                _token(TokenType.KW_RETURN, "return"),
                Identifier(_token(TokenType.MISC_IDENT, "z"), "z"),
            ),
        ]
    )
    benchmark.benchmark_compilation(medium, "Medium Arithmetic")

    # Complex with control flow
    if_stmt = IfStatement(
        _token(TokenType.KW_IF, "if"),
        _create_infix(
            Identifier(_token(TokenType.MISC_IDENT, "x"), "x"),
            ">",
            WholeNumberLiteral(_token(TokenType.LIT_WHOLE_NUMBER, "5"), 5),
        ),
    )

    # Create consequence block
    then_block = BlockStatement(_token(TokenType.OP_GT, ">"))
    then_block.statements = [
        SetStatement(
            _token(TokenType.KW_SET, "set"),
            Identifier(_token(TokenType.MISC_IDENT, "y"), "y"),
            WholeNumberLiteral(_token(TokenType.LIT_WHOLE_NUMBER, "1"), 1),
        )
    ]
    if_stmt.consequence = then_block

    # Create alternative block
    else_block = BlockStatement(_token(TokenType.OP_GT, ">"))
    else_block.statements = [
        SetStatement(
            _token(TokenType.KW_SET, "set"),
            Identifier(_token(TokenType.MISC_IDENT, "y"), "y"),
            WholeNumberLiteral(_token(TokenType.LIT_WHOLE_NUMBER, "0"), 0),
        )
    ]
    if_stmt.alternative = else_block

    complex_prog = Program(
        [
            SetStatement(
                _token(TokenType.KW_SET, "set"),
                Identifier(_token(TokenType.MISC_IDENT, "x"), "x"),
                WholeNumberLiteral(_token(TokenType.LIT_WHOLE_NUMBER, "10"), 10),
            ),
            if_stmt,
            ReturnStatement(
                _token(TokenType.KW_RETURN, "return"),
                Identifier(_token(TokenType.MISC_IDENT, "y"), "y"),
            ),
        ]
    )
    benchmark.benchmark_compilation(complex_prog, "Complex Control Flow")

    # Generate report
    print(benchmark.generate_report())


if __name__ == "__main__":
    run_standard_benchmarks()
